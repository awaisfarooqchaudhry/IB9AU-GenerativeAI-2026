{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkTS9irwKv8xIKZK1fB+tV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaisfarooqchaudhry/IB9AU-GenerativeAI-2026/blob/main/Task5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbf52c74"
      },
      "source": [
        "You can upload a CSV file from your local machine. When prompted, click 'Choose Files' and select the CSV file you want to upload."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f8bf165"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io # Import the io module\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  # Assuming the uploaded file is a CSV, read it into a pandas DataFrame\n",
        "  df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')))\n",
        "\n",
        "print(\"\\nFirst 5 rows of the uploaded CSV file:\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42a609ca"
      },
      "source": [
        "# Task\n",
        "Clean the 'text' column of the `df` DataFrame by extracting URLs into a new 'URL' column and removing them from the original 'text' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "469b6bd7"
      },
      "source": [
        "## Data Cleaning - URL Extraction and Removal\n",
        "\n",
        "### Subtask:\n",
        "Extract URLs from the 'text' column using regular expressions, create a new 'URL' column with the extracted URLs, and then remove these URLs from the original 'text' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7939832f"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract, store, and remove URLs from the 'text' column, I need to define a regular expression pattern and then apply it to each row of the DataFrame. I will use the `re` module for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "047a7bc5"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define a regular expression pattern for URLs\n",
        "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "\n",
        "# Create a new 'URL' column and initialize with empty strings\n",
        "df['URL'] = ''\n",
        "\n",
        "# Iterate through each row to extract URLs and clean the text\n",
        "for index, row in df.iterrows():\n",
        "    text = row['text']\n",
        "    found_urls = url_pattern.findall(text)\n",
        "    if found_urls:\n",
        "        df.at[index, 'URL'] = ', '.join(found_urls) # Store URLs as a comma-separated string\n",
        "        # Remove URLs from the text column\n",
        "        df.at[index, 'text'] = url_pattern.sub('', text).strip()\n",
        "\n",
        "print(\"DataFrame after URL extraction and removal:\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b2e1ea"
      },
      "source": [
        "## Embeddings Generation - Install Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the `sentence-transformers` library, which is necessary for generating sentence embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3adfbaac"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `sentence-transformers` library. The `!pip install` command is used for this purpose, with the `--quiet` flag to keep the output concise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1816fc"
      },
      "source": [
        "print(\"Installing sentence-transformers...\")\n",
        "!pip install sentence-transformers --quiet\n",
        "print(\"sentence-transformers installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c73765df"
      },
      "source": [
        "## Embeddings Generation - Load Model and Generate Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Load the 'all-MiniLM-L6-v2' sentence-transformer model and generate embeddings for the cleaned text data, storing them for later use in semantic search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89790fff"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate embeddings, I will import the `SentenceTransformer` class, load the 'all-MiniLM-L6-v2' model, handle potential NaN values in the 'text' column by filling them with empty strings, and then generate embeddings for the cleaned text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530cece1"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the pre-trained 'all-MiniLM-L6-v2' model\n",
        "print(\"Loading SentenceTransformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Handle potential NaN values in the 'text' column by filling them with empty strings\n",
        "# This ensures the encoder receives valid string inputs.\n",
        "df['text_cleaned'] = df['text'].fillna('')\n",
        "\n",
        "# Generate embeddings for the 'text_cleaned' column\n",
        "print(\"Generating embeddings...\")\n",
        "text_embeddings = model.encode(df['text_cleaned'].tolist(), show_progress_bar=True)\n",
        "print(\"Embeddings generated successfully.\")\n",
        "\n",
        "# Display the shape of the generated embeddings\n",
        "print(f\"Shape of generated embeddings: {text_embeddings.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03eff7ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully generated text embeddings. To make these embeddings easily accessible and associated with their respective text entries for subsequent semantic search operations, they should be added as a new column to the DataFrame `df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95d7f6ea"
      },
      "source": [
        "df['text_embeddings'] = text_embeddings.tolist()\n",
        "\n",
        "print(\"Embeddings successfully added to the DataFrame.\")\n",
        "print(\"DataFrame head with new 'text_embeddings' column:\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d0083b9"
      },
      "source": [
        "## Semantic Search - Similarity Function\n",
        "\n",
        "### Subtask:\n",
        "Define a function that takes a user query, generates its embedding, computes the cosine similarity against all generated text embeddings, and returns the top 5 most similar records along with their similarity scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ed2b74"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the semantic search functionality, I need to define a Python function that takes a user query, generates its embedding, computes cosine similarity with existing text embeddings, and returns the top similar results from the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b42049"
      },
      "source": [
        "from sentence_transformers import util\n",
        "import torch\n",
        "\n",
        "def semantic_search_function(user_query, model, df, text_embeddings, top_n=5):\n",
        "    # Generate embedding for the user query\n",
        "    query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_scores = util.cos_sim(query_embedding, text_embeddings)\n",
        "\n",
        "    # Get the top N similarity scores and their indices\n",
        "    # Squeeze to convert from 1xN tensor to N tensor for topk\n",
        "    top_results = torch.topk(cosine_scores.squeeze(0), k=top_n)\n",
        "\n",
        "    # Prepare results\n",
        "    results = []\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        results.append({\n",
        "            'similarity_score': score.item(),\n",
        "            'record': df.iloc[idx.item()].to_dict() # Convert Series to dict for better display\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"Semantic search function defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "854ebf1f"
      },
      "source": [
        "## Gradio UI - Build Interface\n",
        "\n",
        "### Subtask:\n",
        "Create a Gradio interface that provides a text input for the user's query, uses the semantic search function to find relevant records, and displays the top 5 closest records along with their similarity scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44071d77"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step to building the Gradio interface is to import the `gradio` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4026939"
      },
      "source": [
        "import gradio as gr\n",
        "print(\"Gradio library imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef46262a"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the Gradio interface, I need to define the `search_news` wrapper function, call the `semantic_search_function` within it, format the results, and then create and launch the `gr.Interface`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "063c48a9"
      },
      "source": [
        "def search_news(user_query):\n",
        "    # Call the previously defined semantic_search_function\n",
        "    results = semantic_search_function(user_query, model, df, text_embeddings, top_n=5)\n",
        "\n",
        "    # Format the results into a readable HTML string\n",
        "    html_output = \"\"\n",
        "    if not results:\n",
        "        html_output = \"<p>No results found for your query.</p>\"\n",
        "    else:\n",
        "        html_output = \"<h3>Search Results:</h3>\"\n",
        "        for i, res in enumerate(results):\n",
        "            record = res['record']\n",
        "            similarity_score = res['similarity_score']\n",
        "            html_output += f\"<div>\"\n",
        "            html_output += f\"<p><b>Result {i+1} (Similarity: {similarity_score:.4f}):</b></p>\"\n",
        "            html_output += f\"<p><b>Text:</b> {record.get('text', 'N/A')}</p>\"\n",
        "            if record.get('URL'):\n",
        "                urls = record['URL'].split(', ')\n",
        "                html_output += f\"<p><b>URL(s):</b> \"\n",
        "                for url in urls:\n",
        "                    html_output += f\"<a href='{url}' target='_blank'>{url}</a> \"\n",
        "                html_output += \"</p>\"\n",
        "            html_output += f\"<hr></div>\"\n",
        "\n",
        "    return html_output\n",
        "\n",
        "# Create a Gradio interface\n",
        "news_interface = gr.Interface(\n",
        "    fn=search_news,\n",
        "    inputs=gr.Textbox(lines=2, label='Enter your news query'),\n",
        "    outputs=gr.HTML(label='Search Results'),\n",
        "    title='Semantic News Search',\n",
        "    description='Enter a query to find semantically similar news articles.'\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "print(\"Launching Gradio interface...\")\n",
        "news_interface.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9ffc17"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the performed tasks and provide instructions on how to use the generated Gradio semantic search interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f7203c0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Cleaning:** URLs were successfully extracted from the 'text' column into a new 'URL' column and then removed from the original 'text' column, ensuring a clean text dataset.\n",
        "*   **Library Installation:** The `sentence-transformers` library was successfully installed, which is essential for generating text embeddings.\n",
        "*   **Embeddings Generation:**\n",
        "    *   The 'all-MiniLM-L6-v2' SentenceTransformer model was loaded successfully.\n",
        "    *   `NaN` values in the 'text' column were handled by converting them to empty strings.\n",
        "    *   Embeddings were generated for 16,990 text entries, resulting in 384-dimensional vectors, and stored in a new `text_embeddings` column in the DataFrame.\n",
        "*   **Semantic Search Functionality:** A `semantic_search_function` was defined, capable of taking a user query, generating its embedding, calculating cosine similarity against existing text embeddings, and returning the top 5 most similar records along with their similarity scores.\n",
        "*   **Gradio User Interface:** A Gradio interface was built, allowing users to input a query, which then uses the semantic search function to display the top 5 relevant news articles with their text, URL(s) (as clickable links), and similarity scores in a user-friendly HTML format. The interface was launched with a public URL.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The created Gradio interface provides an accessible and intuitive way for users to perform semantic searches on the news article dataset, leveraging the power of sentence embeddings.\n",
        "*   Consider implementing a feedback mechanism within the Gradio interface to allow users to rate the relevance of search results, which could be used to fine-tune the semantic search model or improve result ranking in the future.\n"
      ]
    }
  ]
}