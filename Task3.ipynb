{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+IZ3rolSEpJ11aPpe7N0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaisfarooqchaudhry/IB9AU-GenerativeAI-2026/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulSc3kEdYcin"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51de7717"
      },
      "source": [
        "# Task\n",
        "Create a PyTorch notebook that downloads the German Credit dataset from \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\", preprocesses it by mapping the target variable, one-hot encoding categorical features, scaling numerical features, and splitting into training/testing sets. Then, define, train, and evaluate a Multi-Layer Perceptron (MLP) model for binary classification, reporting the final test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "865ed5c2"
      },
      "source": [
        "## Imports and Setup\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries such as pandas, numpy, sklearn, and torch. Set up a random seed for reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b425fe"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires importing several libraries and setting a random seed for reproducibility. This code block will handle all these imports and seed settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4979df4b",
        "outputId": "908ac63a-4c45-43ce-caa8-74eb9e60db8f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"Libraries imported and random seeds set.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and random seeds set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa18f7c0"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "### Subtask:\n",
        "Download the dataset from the provided URL. Load the space-separated data into a pandas DataFrame and assign the specified column names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d2a212"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires downloading data from a URL and loading it into a pandas DataFrame with specified column names. This code block will define the URL, column names, and then use pandas to read the data with the correct separator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451b4e38",
        "outputId": "7d23afba-ea92-4579-bcfe-b066677c480c"
      },
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "\n",
        "column_names = [\n",
        "    'checking_status', 'duration', 'credit_history', 'purpose', 'credit_amount',\n",
        "    'savings_status', 'employment_status', 'installment_rate', 'personal_status',\n",
        "    'other_parties', 'residence_duration', 'property_magnitude', 'age',\n",
        "    'other_payment_plans', 'housing', 'existing_credits', 'job', 'num_dependents',\n",
        "    'telephone', 'foreign_worker', 'creditability'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(url, sep=' ', header=None, names=column_names)\n",
        "\n",
        "print(\"Dataset loaded successfully. Displaying the first 5 rows:\")\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully. Displaying the first 5 rows:\n",
            "  checking_status  duration credit_history purpose  credit_amount  \\\n",
            "0             A11         6            A34     A43           1169   \n",
            "1             A12        48            A32     A43           5951   \n",
            "2             A14        12            A34     A46           2096   \n",
            "3             A11        42            A32     A42           7882   \n",
            "4             A11        24            A33     A40           4870   \n",
            "\n",
            "  savings_status employment_status  installment_rate personal_status  \\\n",
            "0            A65               A75                 4             A93   \n",
            "1            A61               A73                 2             A92   \n",
            "2            A61               A74                 2             A93   \n",
            "3            A61               A74                 2             A93   \n",
            "4            A61               A73                 3             A93   \n",
            "\n",
            "  other_parties  ...  property_magnitude age  other_payment_plans housing  \\\n",
            "0          A101  ...                A121  67                 A143    A152   \n",
            "1          A101  ...                A121  22                 A143    A152   \n",
            "2          A101  ...                A121  49                 A143    A152   \n",
            "3          A103  ...                A122  45                 A143    A153   \n",
            "4          A101  ...                A124  53                 A143    A153   \n",
            "\n",
            "  existing_credits   job num_dependents  telephone foreign_worker  \\\n",
            "0                2  A173              1       A192           A201   \n",
            "1                1  A173              1       A191           A201   \n",
            "2                1  A172              2       A191           A201   \n",
            "3                1  A173              2       A191           A201   \n",
            "4                2  A173              2       A191           A201   \n",
            "\n",
            "  creditability  \n",
            "0             1  \n",
            "1             2  \n",
            "2             1  \n",
            "3             1  \n",
            "4             2  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bef4fb0b"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Map the target variable (Creditability) from 1 to 1 and 2 to 0. Identify categorical features and apply one-hot encoding. Split the data into training and testing sets (80/20 ratio). Apply StandardScaler to numerical features, fitting only on the training data and then transforming both training and test sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2403e2c0"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires mapping the target variable, separating features and target, identifying categorical and numerical columns, applying one-hot encoding, splitting data into training and testing sets, and then scaling numerical features. I will perform all these preprocessing steps in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19d7394b",
        "outputId": "4d503bbf-c5fd-4b3e-f728-744ad8d33735"
      },
      "source": [
        "df['creditability'] = df['creditability'].map({1: 1, 2: 0})\n",
        "\n",
        "X = df.drop('creditability', axis=1)\n",
        "y = df['creditability']\n",
        "\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ensure numerical_cols only contains columns present in X_train after one-hot encoding\n",
        "numerical_cols_after_ohe = [col for col in numerical_cols if col in X_train.columns]\n",
        "\n",
        "X_train[numerical_cols_after_ohe] = scaler.fit_transform(X_train[numerical_cols_after_ohe])\n",
        "X_test[numerical_cols_after_ohe] = scaler.transform(X_test[numerical_cols_after_ohe])\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete.\n",
            "Shape of X_train: (800, 48)\n",
            "Shape of X_test: (200, 48)\n",
            "Shape of y_train: (800,)\n",
            "Shape of y_test: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11b0729d"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been preprocessed and split. Now, to prepare for PyTorch model training, I need to convert the numpy arrays (which pandas DataFrames and Series effectively become for these operations) into PyTorch tensors. This is a necessary step before defining and training the MLP model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b84bb94",
        "outputId": "20c79d13-fc7b-491f-aff9-40ab2ced182b"
      },
      "source": [
        "X_train_tensor = torch.tensor(X_train.values.astype(np.float32), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test.values.astype(np.float32), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"Data converted to PyTorch tensors.\")\n",
        "print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
        "print(f\"y_train_tensor shape: {y_train_tensor.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data converted to PyTorch tensors.\n",
            "X_train_tensor shape: torch.Size([800, 48])\n",
            "y_train_tensor shape: torch.Size([800, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "386f4246"
      },
      "source": [
        "## Convert to PyTorch Tensors and Create DataLoaders\n",
        "\n",
        "### Subtask:\n",
        "Create TensorDatasets and DataLoaders for the training and testing sets to enable efficient batch processing during model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e214ce6"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to create TensorDatasets and DataLoaders as per the subtask instructions. This involves combining the feature and target tensors into datasets and then wrapping them in DataLoaders for batch processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f2f5dbf",
        "outputId": "7a41ccd7-3a19-40c9-8e70-50283da9c7fa"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"TensorDatasets and DataLoaders created.\")\n",
        "print(f\"Training DataLoader has {len(train_loader)} batches of size {batch_size}.\")\n",
        "print(f\"Testing DataLoader has {len(test_loader)} batches of size {batch_size}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDatasets and DataLoaders created.\n",
            "Training DataLoader has 13 batches of size 64.\n",
            "Testing DataLoader has 4 batches of size 64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1796e486"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "### Subtask:\n",
        "Define a Multi-Layer Perceptron (MLP) model using PyTorch's nn.Module. The model should have at least two hidden layers with ReLU activation functions, and the final output layer should use a Sigmoid activation for binary classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0290d77"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the MLP model class, instantiate it with the correct input and output dimensions, and then print its architecture as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173d7f34",
        "outputId": "dddc6099-af3c-4b3f-f861-11eddafd004c"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 32\n",
        "output_dim = 1\n",
        "\n",
        "model = MLP(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "\n",
        "print(\"MLP model defined.\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model defined.\n",
            "MLP(\n",
            "  (fc1): Linear(in_features=48, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bc7ddea"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Subtask:\n",
        "Set up the training loop for the defined MLP model. Use nn.BCELoss as the criterion and Adam as the optimizer. Train the model for 50 epochs, printing the loss every 5 epochs to monitor progress.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1208c727"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the loss function and optimizer, then implement the training loop as specified, iterating through epochs and batches, performing forward/backward passes, and updating weights, while printing the loss periodically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12af855a",
        "outputId": "cc0bd2a7-22de-42bb-a523-5ec8902915e7"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "print(f\"Starting model training for {epochs} epochs...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training for 50 epochs...\n",
            "Epoch 5/50, Loss: 0.5496\n",
            "Epoch 10/50, Loss: 0.4597\n",
            "Epoch 15/50, Loss: 0.4099\n",
            "Epoch 20/50, Loss: 0.3585\n",
            "Epoch 25/50, Loss: 0.3082\n",
            "Epoch 30/50, Loss: 0.2600\n",
            "Epoch 35/50, Loss: 0.2147\n",
            "Epoch 40/50, Loss: 0.1660\n",
            "Epoch 45/50, Loss: 0.1247\n",
            "Epoch 50/50, Loss: 0.0971\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd83cde"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained MLP model on the test set. Calculate and print the accuracy of the model on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b905effd"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to evaluate the trained model on the test set and calculate its accuracy. This code will put the model in evaluation mode, iterate through the test data, make predictions, and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e08343b7",
        "outputId": "5f5122d6-46c7-4a4f-8037-cb79378aa06d"
      },
      "source": [
        "model.eval() # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs > 0.5).float() # Convert probabilities to binary predictions (0 or 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 72.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40aff428"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the completed task, confirming that the PyTorch notebook for German Credit binary classification has been created and evaluated according to the specified requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15b46f1f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The final test accuracy achieved by the Multi-Layer Perceptron (MLP) model was 72.50%.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Data Acquisition and Preparation**: The German Credit dataset, containing 1000 entries, was successfully downloaded and loaded into a pandas DataFrame with 21 specified columns.\n",
        "*   **Target Variable Preprocessing**: The 'creditability' target variable was remapped from $\\{1, 2\\}$ to $\\{1, 0\\}$ for binary classification compatibility.\n",
        "*   **Feature Engineering**: Categorical features were identified and transformed using one-hot encoding, while numerical features were scaled using `StandardScaler` fitted on the training data. The dataset was split into training (800 samples) and testing (200 samples) sets.\n",
        "*   **PyTorch Data Preparation**: Preprocessed data was successfully converted into PyTorch tensors and organized into `TensorDatasets` and `DataLoaders` with a batch size of 64, preparing it for batch-wise training.\n",
        "*   **MLP Model Architecture**: A Multi-Layer Perceptron (MLP) model was defined with an input layer matching the 48 features, two hidden layers of 64 and 32 neurons respectively, both utilizing ReLU activation, and a final output layer with a Sigmoid activation for binary classification.\n",
        "*   **Model Training Progress**: The MLP model was trained for 50 epochs using `nn.BCELoss` as the criterion and Adam optimizer with a learning rate of 0.001. The training loss consistently decreased from an average loss of approximately 0.5496 at Epoch 5 to 0.0971 at Epoch 50, indicating successful learning.\n",
        "*   **Model Performance**: The trained model achieved an accuracy of 72.50% on the unseen test set.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The current model achieves a reasonable baseline accuracy of 72.50%. Further optimization could involve hyperparameter tuning (e.g., learning rate, hidden layer sizes, batch size), exploring different activation functions, or regularization techniques to potentially improve performance.\n",
        "*   Given that the dataset is for credit risk assessment, a deeper analysis of false positives and false negatives (confusion matrix, precision, recall, F1-score) would provide more critical insights into the model's decision-making and its implications for lending decisions.\n"
      ]
    }
  ]
}