{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNofVLjj6jRTk4+pHyEIQlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaisfarooqchaudhry/IB9AU-GenerativeAI-2026/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l89ynsoTqML"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3f0dab5"
      },
      "source": [
        "## PyTorch Autograd Example\n",
        "\n",
        "This notebook demonstrates how to use PyTorch's Autograd engine to compute gradients for a given mathematical expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6d003e4"
      },
      "source": [
        "### 1. Import `torch`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96197236"
      },
      "source": [
        "# Import the PyTorch library, which is essential for tensor operations and automatic differentiation.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aafaf3"
      },
      "source": [
        "### 2. Create Tensors for `w`, `x`, and `b`\n",
        "\n",
        "We define the input variables as PyTorch tensors and set `requires_grad=True` to signal PyTorch to track operations on these tensors for gradient computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ef9579f",
        "outputId": "32c88b36-6c22-4d4e-c4bc-8909f45af10d"
      },
      "source": [
        "# Define the initial values for w, x, and b as PyTorch tensors.\n",
        "# setting 'requires_grad=True' tells PyTorch to track gradients for these tensors.\n",
        "w = torch.tensor(2.0, dtype=torch.float64, requires_grad=True)\n",
        "x = torch.tensor(4.0, dtype=torch.float64, requires_grad=True)\n",
        "b = torch.tensor(1.5, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "\n",
        "print(f\"Initial w: {w}\")\n",
        "print(f\"Initial x: {x}\")\n",
        "print(f\"Initial b: {b}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial w: 2.0\n",
            "Initial x: 4.0\n",
            "Initial b: 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d6eda8"
      },
      "source": [
        "### 3. Compute `z = sigmoid(w * x^2) + (1 / b^3)`\n",
        "\n",
        "We implement the given mathematical expression using PyTorch tensor operations. PyTorch automatically builds a computation graph in the background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "253a9cd0",
        "outputId": "2b17ca8d-67b0-4115-f1a6-1c85a9c3ac01"
      },
      "source": [
        "# Compute the intermediate value for the sigmoid function argument.\n",
        "arg_sigmoid = w * (x ** 2)\n",
        "\n",
        "# Compute z using the defined expression.\n",
        "# torch.sigmoid() applies the sigmoid activation function.\n",
        "# torch.pow(b, 3) calculates b raised to the power of 3.\n",
        "z = torch.sigmoid(arg_sigmoid) + (1 / (b ** 3))\n",
        "\n",
        "# Print the numerical value of z.\n",
        "print(f\"Computed value of z: {z.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed value of z: 1.2962962962962836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07d31fa6"
      },
      "source": [
        "### 4. Call `z.backward()` and Print Gradients\n",
        "\n",
        "Calling `z.backward()` computes the gradients of `z` with respect to all tensors that have `requires_grad=True` and were part of the computation graph leading to `z`. The gradients are then stored in the `.grad` attribute of these tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7484e26c",
        "outputId": "85fbc46b-7bb4-430f-82db-597ca3231e51"
      },
      "source": [
        "# Perform backpropagation to compute the gradients of z with respect to w, x, and b.\n",
        "# This populates the .grad attribute for tensors that require gradients.\n",
        "z.backward()\n",
        "\n",
        "# Print the computed gradients.\n",
        "print(f\"Gradient of z with respect to w (dz/dw): {w.grad}\")\n",
        "print(f\"Gradient of z with respect to x (dz/dx): {x.grad}\")\n",
        "print(f\"Gradient of z with respect to b (dz/db): {b.grad}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of z with respect to w (dz/dw): 2.0250467969162598e-13\n",
            "Gradient of z with respect to x (dz/dx): 2.0250467969162598e-13\n",
            "Gradient of z with respect to b (dz/db): -0.5925925925925926\n"
          ]
        }
      ]
    }
  ]
}